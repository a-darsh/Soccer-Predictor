{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1iPtiJqnDnsgI548FMpyu1n7kpuQWRpGU","authorship_tag":"ABX9TyNo9fUOxd4pgFwynuR9qgf4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Change to Project Folder\n","cd '/content/drive/Othercomputers/My MacBook Pro/Academic Projects/DB Project'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"yQ5hs3bFncLa","executionInfo":{"status":"error","timestamp":1700265640951,"user_tz":360,"elapsed":210,"user":{"displayName":"Adarsh Ramesh","userId":"15003989314018566872"}},"outputId":"43c14824-64b6-4e5e-8a84-6b0c566f77ee"},"execution_count":5,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-9223f149fade>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    cd '/content/drive/Othercomputers/My MacBook Pro/Academic Projects/DB Project'\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["# install transfomer library\n","!pip install transformers"],"metadata":{"id":"HaZ-9Edwnkdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import Statements\n","import pandas as pd\n","import re\n","import pandas as pd\n","import re\n","from transformers import TFAutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","import numpy as np\n","import tensorflow as tf\n","from tqdm import tqdm"],"metadata":{"id":"YRyAQoX6npdW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading Sentiment Data\n","\n","df_twitter1 = pd.read_csv('2020-07-09 till 2020-09-19.csv')\n","df_twitter2 = pd.read_csv('2020-09-20 till 2020-10-13.csv')\n","\n","# Concatenating them\n","df = pd.concat([df_twitter1, df_twitter2])\n","\n","df"],"metadata":{"id":"6FCWU5omnufh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading Match Result Data\n","\n","df_match_result = pd.read_csv('premier-league-matches.csv')\n","\n","# Convert the 'Date' column to datetime\n","df_match_result['Date'] = pd.to_datetime(df_match_result['Date'])\n","\n","# Define  date range based on available sentiment data\n","start_date = '2020-07-09'\n","end_date = '2020-10-13'\n","\n","# Filter the data\n","df_match_result_sliced = df_match_result[(df_match_result['Date'] >= start_date) & (df_match_result['Date'] <= end_date)]\n","\n","\n","df_match_result_sliced"],"metadata":{"id":"2yTbPlDVn9Vr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Modifying Sentiment Data - Getting in sentiment data spanning over previous 7 days for every single match date in Match Result data\n","\n","# Convert your DataFrame date column to datetime\n","df['created_at'] = pd.to_datetime(df['created_at'])\n","\n","#  list of dates\n","date_list = df_match_result_sliced.Date.unique()\n","date_list = pd.to_datetime(date_list)  # convert list to datetime\n","\n","# Function to check if a date is within 7 days of any date in the list\n","def is_within_7_days(row_date):\n","    return any(abs(row_date - list_date) <= pd.Timedelta(days=7) for list_date in date_list)\n","\n","# Apply the function to filter the DataFrame\n","df_sentiment = df[df['created_at'].apply(is_within_7_days)]\n","\n","df_sentiment"],"metadata":{"id":"XlzeH_sdoTDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clean the tweets\n","def clean_tweet(tweet):\n","    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n","    tweet = re.sub(r'\\@\\w+', '', tweet)\n","    tweet = re.sub(r'#', '', tweet)\n","    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","    return tweet\n","\n","\n","df_sentiment['clean_text'] = df_sentiment['text'].apply(clean_tweet)"],"metadata":{"id":"m4EXB5Qho2fj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Loading\n","\n","# Initialize tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n","model = TFAutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')"],"metadata":{"id":"L_1w93VmoyJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction Code - Generating Sentiments (Score as well as label) using Model\n","\n","def batch_predict(texts, batch_size=1000):  # Increased batch size\n","    predictions = []\n","    for i in tqdm(range(0, len(texts), batch_size)):  # tqdm for progress tracking\n","        batch = texts[i:i+batch_size]\n","        encoded_batch = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"tf\")\n","        outputs = model(encoded_batch['input_ids'], attention_mask=encoded_batch['attention_mask'])\n","        batch_predictions = tf.nn.softmax(outputs.logits, axis=-1)\n","        predictions.extend(batch_predictions.numpy().tolist())\n","    return predictions\n","\n","# Predict sentiments in batches\n","sentiment_scores = batch_predict(df_sentiment['clean_text'].tolist(), batch_size=1000)  # Use .tolist() directly\n","\n","# Adding score to df\n","df_sentiment['sentiment_scores'] = sentiment_scores\n","\n","# Extract the sentiment labels from the sentiment scores\n","df_sentiment['sentiment'] = [np.argmax(score) for score in sentiment_scores]\n","df_sentiment['sentiment label'] = df_sentiment['sentiment'].map({0: 'Negative', 1: 'Neutral', 2: 'Positive'})\n","\n","\n","df_sentiment"],"metadata":{"id":"seMwd0BxpTjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving Sentiments Data\n","df_sentiment['created_at'] = pd.to_datetime(df_sentiment['created_at'], errors='coerce')\n","df_sentiment.to_csv('twitter_sentiment_analysis.csv', index=False)\n","\n","# Saving Match Result Data\n","df_match_result_sliced.to_csv('match_result.csv', index=False)"],"metadata":{"id":"M9xPwW3VpWLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading Bets Data\n","\n","df_bet1 = pd.read_csv('E0.csv')\n","\n","df_bet2 = pd.read_csv('2019-2020.csv')\n","\n","# Concatenating them\n","df_bet = pd.concat([df_bet1, df_bet2])\n","\n","df_bet"],"metadata":{"id":"WjmgVr9BpfCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting Relevant Bets Data\n","\n","# Convert the 'Date' column to datetime\n","df_bet['Date'] = pd.to_datetime(df_bet['Date'])\n","\n","# Define date range\n","start_date = '2020-07-09'\n","end_date = '2020-10-13'\n","\n","# Filter the data\n","df_bets_data = df_bet[(df_bet['Date'] >= start_date) & (df_bet['Date'] <= end_date)]\n","\n","df_bets_data.to_csv('Bets_Data.csv', index=False)"],"metadata":{"id":"NEZa5ByfppBu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read all data\n","df_sentiment = pd.read_csv('twitter_sentiment_analysis.csv')\n","df_bets_data = pd.read_csv('Bets_Data.csv')\n","df_match_result = pd.read_csv('match_result.csv')"],"metadata":{"id":"b3cZeSglqvy4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Team Mapping to maintain uniformity accross Datasets\n","team_name_mapping = {\n","    'West Brom': 'West Bromwich Albion',\n","    'Tottenham': 'Tottenham Hotspur',\n","    'Brighton': 'Brighton & Hove Albion',\n","    'Sheffield United': 'Sheffield United',\n","    'Sheffield Utd': 'Sheffield United',\n","    'Everton': 'Everton',\n","    'Leeds': 'Leeds United',\n","    'Leeds United': 'Leeds United',\n","    'Man United': 'Manchester United',\n","    'Manchester Utd': 'Manchester United',\n","    'Arsenal': 'Arsenal',\n","    'Southampton': 'Southampton',\n","    'Newcastle': 'Newcastle United',\n","    'Newcastle Utd': 'Newcastle United',\n","    'Chelsea': 'Chelsea',\n","    'Leicester': 'Leicester City',\n","    'Leicester City': 'Leicester City',\n","    'Aston Villa': 'Aston Villa',\n","    'Wolves': 'Wolverhampton Wanderers',\n","    'Crystal Palace': 'Crystal Palace',\n","    'Burnley': 'Burnley',\n","    'Man City': 'Manchester City',\n","    'Manchester City': 'Manchester City',\n","    'West Ham': 'West Ham United',\n","    'Fulham': 'Fulham',\n","    'Liverpool': 'Liverpool',\n","    'Bournemouth': 'AFC Bournemouth',\n","    'Norwich': 'Norwich City',\n","    'Norwich City': 'Norwich City',\n","    'Watford': 'Watford'\n","}\n","\n","\n","df_bets_data['HomeTeam'] = df_bets_data['HomeTeam'].map(team_name_mapping).fillna(df_bets_data['HomeTeam'])\n","df_bets_data['AwayTeam'] = df_bets_data['AwayTeam'].map(team_name_mapping).fillna(df_bets_data['AwayTeam'])\n","\n","\n","df_match_result['Home'] = df_match_result['Home'].map(team_name_mapping).fillna(df_match_result['Home'])\n","df_match_result['Away'] = df_match_result['Away'].map(team_name_mapping).fillna(df_match_result['Away'])\n"],"metadata":{"id":"6mmsotcNrn2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merging Match Result and Bets Data\n","\n","# Convert dates to datetime objects for accurate merging\n","df_match_result['Date'] = pd.to_datetime(df_match_result['Date'])\n","df_bets_data['Date'] = pd.to_datetime(df_bets_data['Date'])\n","\n","# Merge df_match_result with df_bets_data\n","merged_df = pd.merge(df_match_result, df_bets_data,\n","                     left_on=['Date', 'Home', 'Away'],\n","                     right_on=['Date', 'HomeTeam', 'AwayTeam'])\n","merged_df"],"metadata":{"id":"L4NynpQerfj0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfnyT6WbnWrr"},"outputs":[],"source":["# Merging merge_df with the sentiment data - Averaging out across the  relevant sentiment data(7 days)\n","\n","# Dropping redundant columns\n","merged_df.drop(columns=['HomeTeam', 'AwayTeam'], inplace=True)\n","\n","# Rename betting odds columns for clarity\n","merged_df.rename(columns={'AvgH': 'AvgOdds_HomeWin', 'AvgD': 'AvgOdds_Draw', 'AvgA': 'AvgOdds_AwayWin'}, inplace=True)\n","\n","\n","# Cell execution start\n","def calculate_sentiment_scores(df, team_name, start_date):\n","    end_date = start_date - pd.Timedelta(days=7)\n","    team_sentiments = df[(df['file_name'] == team_name) &\n","                         (df['created_at'] >= end_date) &\n","                         (df['created_at'] < start_date)]\n","\n","    # Initialize sums for each sentiment\n","    positive_sum = neutral_sum = negative_sum = 0\n","\n","    # Check if there are sentiment data points available\n","    if not team_sentiments.empty:\n","        # Debugging: Print first few sentiment scores\n","        print(team_sentiments['sentiment_scores'].head())\n","\n","        # Assuming sentiment_scores is a string representation of a list\n","        for sentiment_score_str in team_sentiments['sentiment_scores']:\n","            try:\n","                sentiment_scores = eval(sentiment_score_str)  # Convert string to list\n","                positive_sum += sentiment_scores[2]\n","                neutral_sum += sentiment_scores[1]\n","                negative_sum += sentiment_scores[0]\n","            except Exception as e:\n","                print(f\"Error processing sentiment score {sentiment_score_str}: {e}\")\n","\n","    return positive_sum, neutral_sum, negative_sum\n","\n","\n","# Cell execution start\n","for index, row in merged_df.iterrows():\n","    match_date = row['Date']\n","    home_team = row['Home']\n","    away_team = row['Away']\n","\n","    home_positive, home_neutral, home_negative = calculate_sentiment_scores(df_sentiment, home_team, match_date)\n","    away_positive, away_neutral, away_negative = calculate_sentiment_scores(df_sentiment, away_team, match_date)\n","\n","    merged_df.at[index, 'HomeTeam_PositiveSentiment'] = home_positive\n","    merged_df.at[index, 'HomeTeam_NeutralSentiment'] = home_neutral\n","    merged_df.at[index, 'HomeTeam_NegativeSentiment'] = home_negative\n","    merged_df.at[index, 'AwayTeam_PositiveSentiment'] = away_positive\n","    merged_df.at[index, 'AwayTeam_NeutralSentiment'] = away_neutral\n","    merged_df.at[index, 'AwayTeam_NegativeSentiment'] = away_negative\n","\n","\n","# Cell execution start\n","print_columns = ['Date', 'Home', 'Away',\n","                 'HomeTeam_PositiveSentiment', 'HomeTeam_NeutralSentiment', 'HomeTeam_NegativeSentiment',\n","                 'AwayTeam_PositiveSentiment', 'AwayTeam_NeutralSentiment', 'AwayTeam_NegativeSentiment',\n","                 'AvgOdds_HomeWin', 'AvgOdds_Draw', 'AvgOdds_AwayWin',\n","                 'HomeGoals', 'AwayGoals']\n","\n","# Displaying the merged dataframe with specified columns\n","merged_df = merged_df[print_columns]\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["# Scalling the dataset\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Columns to be scaled\n","sentiment_cols = ['HomeTeam_PositiveSentiment', 'HomeTeam_NeutralSentiment', 'HomeTeam_NegativeSentiment',\n","                  'AwayTeam_PositiveSentiment', 'AwayTeam_NeutralSentiment', 'AwayTeam_NegativeSentiment']\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","# Fit and transform the sentiment columns\n","merged_df[sentiment_cols] = scaler.fit_transform(merged_df[sentiment_cols])\n","\n","\n","# Cell execution start\n","for col in sentiment_cols:\n","    # Calculate the mean of the column excluding zeros\n","    mean_value = merged_df[merged_df[col] != 0][col].mean()\n","\n","    # Replace zeros with the mean value\n","    merged_df[col] = merged_df[col].replace(0, mean_value)\n","\n","\n","merged_df\n","\n","\n"],"metadata":{"id":"2pdRbaIUr99H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving the training data\n","merged_df.to_csv('train_data.csv')\n"],"metadata":{"id":"S3QYQNJQsNpW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r Data Data.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ncN_5M8Mtdod","executionInfo":{"status":"ok","timestamp":1700265627422,"user_tz":360,"elapsed":161,"user":{"displayName":"Adarsh Ramesh","userId":"15003989314018566872"}},"outputId":"815b9ab0-151c-4ba5-afdf-fc3ad70d4d61"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\tzip warning: name not matched: Data.zip\n","\n","zip error: Nothing to do! (try: zip -r Data . -i Data.zip)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eBcx-OQSuozW"},"execution_count":null,"outputs":[]}]}